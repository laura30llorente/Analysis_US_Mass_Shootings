{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep LLMs - Proceso de Extracción y Limpieza de Ubicaciones en Resúmenes de Tiroteos**\n",
        "\n",
        "Al analizar los datos del dataset de tiroteos, me di cuenta de que había tiroteos en los que aparecía vacío el campo de Location, a pesar de mencionarse la ubicación del tiroteo en el campo Summary.\n",
        "\n",
        "El objetivo principal de este apartado es identificar y extraer automáticamente las ubicaciones mencionadas en los resúmenes de incidentes de tiroteos, y posteriormente limpiar y corregir cualquier anomalía en los datos extraídos.\n",
        "\n",
        "Una vez obtenidas estas ubiaciones, se añadirán en una nueva columna del dataframe para su uso en posteriores análisis.\n",
        "\n",
        "También, podremos comprobar si la ubicación extraída del resumen coincide con la localización de aquellos tiroteos que no tienen el campo Location vacío.\n",
        "\n",
        "Para llevar a cabo esta tarea, hemos utilizado una combinación de herramientas de procesamiento de lenguaje natural (NLP) y técnicas de manipulación de datos.\n",
        "\n",
        "A continuación, mencionamos las principales herramientas y métodos utilizados:\n",
        "\n",
        "- Python y Pandas:\n",
        "\n",
        "  - Utilizamos Python como el lenguaje principal para el procesamiento de datos.\n",
        "\n",
        "  - La biblioteca Pandas se empleó para la carga, manipulación y almacenamiento de datos en formato CSV.\n",
        "\n",
        "- Transformers de Hugging Face:\n",
        "\n",
        "  - Empleamos el modelo *dbmdz/bert-large-cased-finetuned-conll03-english*, un modelo de transformers preentrenado para el reconocimiento de entidades nombradas (NER), capaz de identificar entidades como ubicaciones, nombres de personas y organizaciones en el texto.\n",
        "\n",
        "  - La función pipeline de Hugging Face se utilizó para crear un pipeline de NER, que facilita la identificación y extracción de ubicaciones.\n",
        "\n",
        "- Tokenización y Limpieza de Datos:\n",
        "\n",
        "  - Los modelos de transformers a menudo descomponen palabras en subpalabras, marcando las continuaciones con un prefijo ##. Implementamos una lógica específica para unir estas subpalabras y formar palabras completas.\n",
        "\n",
        "  - Creamos una función personalizada para procesar los tokens, eliminando caracteres no deseados y uniendo subpalabras adecuadamente.\n",
        "\n",
        "- Procesamiento por Lotes:\n",
        "  - Para manejar de manera eficiente el procesamiento de grandes volúmenes de texto, dividimos el dataset en lotes y procesamos cada lote de manera iterativa."
      ],
      "metadata": {
        "id": "x8SPJnMC5X3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRUEBA 1**\n",
        "\n",
        "Este código realiza un análisis de texto utilizando el modelo de reconocimiento de entidades nombradas (NER) de Hugging Face para extraer ubicaciones de los resúmenes de incidentes de tiroteos.\n",
        "El script carga el dataset, utiliza el modelo de transformers preentrenado para identificar y extraer las ubicaciones mencionadas en los resúmenes, y guarda los resultados en un nuevo archivo CSV.\n",
        "\n",
        "**Problema:**\n",
        "Durante el proceso de extracción de ubicaciones utilizando modelos de transformers como BERT, es común encontrar caracteres ## delante de algunas palabras. Esto se debe al proceso de tokenización de subpalabras utilizado por estos modelos.\n",
        "Los tokens que comienzan con ## representan subpalabras que se deben unir a la palabra anterior. Por ejemplo, \"Mandalay Bay\" es tokenizado como \"Man ##ada ##lay Bay\", por lo que debemos solucionar este problema para poder emplear bien los datos."
      ],
      "metadata": {
        "id": "fqyfu-MJ55wV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "F5K0RqWctDSW",
        "outputId": "f84f6b94-4c94-422a-92a4-fb442047f2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Summary  \\\n",
            "0  Devin Patrick Kelley, 26, an ex-air force offi...   \n",
            "1  Scott Allen Ostrem, 47, walked into a Walmart ...   \n",
            "2  Radee Labeeb Prince, 37, fatally shot three pe...   \n",
            "3  Stephen Craig Paddock, opened fire from the 32...   \n",
            "4  Jimmy Lam, 38, fatally shot three coworkers an...   \n",
            "\n",
            "              Extracted_Location  \n",
            "0                          Texas  \n",
            "1                  Denver Denver  \n",
            "2  Baltimore Wilmington Delaware  \n",
            "3            Man ##ada ##lay Bay  \n",
            "4                  San Francisco  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3338e476-7d44-4512-93b1-cd88b08f17b4\", \"Extracted_Locations.csv\", 145558)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Importamos las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Cargamos el dataset\n",
        "file_path = 'Mass Shootings Dataset Ver 5.csv'  # Ruta del archivo CSV subido a Google Colab\n",
        "df = pd.read_csv(file_path, encoding='latin1')  # Cargamos el CSV con codificación 'latin1'\n",
        "\n",
        "# Creamos el pipeline de NER usando un modelo preentrenado y GPU si está disponible\n",
        "device = 0 if torch.cuda.is_available() else -1    # Usamos GPU si está disponible, de lo contrario usar CPU\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", device=device)\n",
        "\n",
        "# Definimos una función para extraer ubicaciones de los summaries\n",
        "def extract_locations_batch(texts):\n",
        "    all_locations = []  # Lista para almacenar todas las ubicaciones extraídas\n",
        "    for text in texts:\n",
        "        ner_results = ner_pipeline(text)  # Aplicamos el modelo NER al texto\n",
        "        # Filtramos las entidades identificadas como ubicaciones (LOC) y la unimos en una cadena\n",
        "        locations = [result['word'] for result in ner_results if 'LOC' in result['entity']]\n",
        "        all_locations.append(' '.join(locations))  # Añadimos las ubicaciones a la lista\n",
        "    return all_locations  # Devolvemos la lista de ubicaciones extraídas\n",
        "\n",
        "# Dividimos el dataset en lotes y acumulamos los resultados\n",
        "batch_size = 10  # Tamaño del lote\n",
        "all_extracted_locations = []  # Lista para acumular todas las ubicaciones extraídas\n",
        "\n",
        "# Procesamos el dataset en lotes\n",
        "for start in range(0, len(df), batch_size):\n",
        "    end = min(start + batch_size, len(df))  # Definimos el rango del lote actual\n",
        "    batch_texts = df['Summary'][start:end].tolist()  # Extraemos el texto del lote actual\n",
        "    # Extraemos las ubicaciones del lote actual y acumulamos los resultados\n",
        "    all_extracted_locations.extend(extract_locations_batch(batch_texts))\n",
        "\n",
        "# Añadimos la columna de ubicaciones extraídas al dataframe\n",
        "df['Extracted_Location'] = all_extracted_locations\n",
        "\n",
        "# Guardamos el dataframe resultante en un archivo CSV\n",
        "output_file_path = 'Extracted_Locations.csv'\n",
        "df.to_csv(output_file_path, index=False)  # Guardamos el dataframe en un archivo CSV sin el índice\n",
        "\n",
        "# Mostramos las primeras filas del dataframe con la nueva columna\n",
        "print(df[['Summary', 'Extracted_Location']].head())\n",
        "\n",
        "# Descargamos el archivo CSV\n",
        "from google.colab import files  # Importamos la biblioteca para descargar archivos en Google Colab\n",
        "files.download('Extracted_Locations.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRUEBA 2**\n",
        "\n",
        "Este código realiza un análisis de texto utilizando el modelo de reconocimiento de entidades nombradas (NER) de Hugging Face para extraer y limpiar ubicaciones de los resúmenes de incidentes de tiroteos.\n",
        "El script carga el dataset, utiliza un modelo de transformers preentrenado para identificar y extraer las ubicaciones mencionadas en los resúmenes, limpia los tokens de subpalabras, y guarda los resultados en un nuevo archivo CSV.\n",
        "\n",
        "**Problema:**\n",
        "Hemos eliminado los caracteres ## delante de las palabras, pero ahora estas subpalabras no se unen adecuadamente y resultan en palabras separadas por espacios. Por ejemplo, \"Man ##ada ##lay Bay\" ha pasado a ser \"Man ada lay Bay\", pero todavía no es \"Mandalay Bay\", que es lo que buscamos para el correcto análisis en R de las localizaciones."
      ],
      "metadata": {
        "id": "EHJUpOLm8QDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Cargamos el dataset\n",
        "file_path = 'Mass Shootings Dataset Ver 5.csv'  # Ruta del archivo CSV en Google Colab\n",
        "df = pd.read_csv(file_path, encoding='latin1')  # Cargamos el CSV con codificación 'latin1'\n",
        "\n",
        "# Creamos el pipeline de NER usando un modelo preentrenado y GPU si está disponible\n",
        "device = 0 if torch.cuda.is_available() else -1  # Usamos GPU si está disponible, de lo contrario usar CPU\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", device=device)\n",
        "\n",
        "# Definimos una función para extraer y limpiar ubicaciones de los summaries\n",
        "def extract_and_clean_locations(text):\n",
        "    ner_results = ner_pipeline(text)  # Aplicamos el modelo NER al texto\n",
        "    locations = []\n",
        "    for result in ner_results:\n",
        "        if 'LOC' in result['entity']:\n",
        "            # Limpiamos los tokens de ubicaciones, eliminando los caracteres '#' de subpalabras\n",
        "            word = result['word'].replace('#', '')\n",
        "            locations.append(word)\n",
        "\n",
        "    # Unimos subpalabras\n",
        "    cleaned_locations = ' '.join(locations).replace(' ##', '').replace('##', '')\n",
        "    return cleaned_locations  # Devolvemos las ubicaciones limpias\n",
        "\n",
        "# Dividimos el dataset en lotes y acumulamos los resultados\n",
        "batch_size = 10  # Tamaño del lote\n",
        "all_extracted_locations = []  # Lista para acumular todas las ubicaciones extraídas y limpiadas\n",
        "\n",
        "# Procesamos el dataset en lotes\n",
        "for start in range(0, len(df), batch_size):\n",
        "    end = min(start + batch_size, len(df))  # Definimos el rango del lote actual\n",
        "    batch_texts = df['Summary'][start:end].tolist()  # Extraemos el texto del lote actual\n",
        "    # Extraemos y limpiamos las ubicaciones del lote actual y acumulamos los resultados\n",
        "    all_extracted_locations.extend([extract_and_clean_locations(text) for text in batch_texts])\n",
        "\n",
        "# Añadimos la columna de ubicaciones extraídas y limpiadas al dataframe\n",
        "df['Extracted_Location'] = all_extracted_locations\n",
        "\n",
        "# Guardamos el dataframe resultante en un archivo CSV\n",
        "output_file_path = 'Extracted_Locations_Clean.csv'\n",
        "df.to_csv(output_file_path, index=False)  # Guardamos el dataframe en un archivo CSV sin el índice\n",
        "\n",
        "# Mostramos las primeras filas del dataframe con la nueva columna\n",
        "print(df[['Summary', 'Extracted_Location']].head())\n",
        "\n",
        "# Descargamos el archivo CSV\n",
        "from google.colab import files  # Importamos la biblioteca para descargar archivos en Google Colab\n",
        "files.download('Extracted_Locations_Clean.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "0kune5rXxATl",
        "outputId": "6c3df9aa-bbe9-4323-d907-f8d3a4aba4dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Summary  \\\n",
            "0  Devin Patrick Kelley, 26, an ex-air force offi...   \n",
            "1  Scott Allen Ostrem, 47, walked into a Walmart ...   \n",
            "2  Radee Labeeb Prince, 37, fatally shot three pe...   \n",
            "3  Stephen Craig Paddock, opened fire from the 32...   \n",
            "4  Jimmy Lam, 38, fatally shot three coworkers an...   \n",
            "\n",
            "              Extracted_Location  \n",
            "0                          Texas  \n",
            "1                  Denver Denver  \n",
            "2  Baltimore Wilmington Delaware  \n",
            "3                Man ada lay Bay  \n",
            "4                  San Francisco  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6328fa87-619b-4ea1-b973-d2abc93c2cba\", \"Extracted_Locations_Clean.csv\", 145288)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRUEBA 3**\n",
        "\n",
        "Este código utiliza el modelo de reconocimiento de entidades nombradas (NER) de Hugging Face para extraer y limpiar ubicaciones de los resúmenes de incidentes de tiroteos.\n",
        "El script carga el dataset, utiliza un modelo de transformers preentrenado para identificar y extraer las ubicaciones mencionadas en los resúmenes, limpia los tokens de subpalabras, une las palabras correctamente y guarda los resultados en un nuevo archivo CSV.\n",
        "\n",
        "Ya hemos solucionado todos los problemas surgidos y hemos obtenido, en aquellos tiroteos que mencionan la ubicación en el resumen, el lugar donde ocurrió el incidente."
      ],
      "metadata": {
        "id": "LP3E_bgI9Pat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Cargamos el dataset\n",
        "file_path = 'Mass Shootings Dataset Ver 5.csv'  # Ruta del archivo CSV en Google Colab\n",
        "df = pd.read_csv(file_path, encoding='latin1')  # Cargamos el CSV con codificación 'latin1'\n",
        "\n",
        "# Creamos el pipeline de NER usando un modelo preentrenado y GPU si está disponible\n",
        "device = 0 if torch.cuda.is_available() else -1  # Usamos GPU si está disponible, de lo contrario usar CPU\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", device=device)\n",
        "\n",
        "# Definimos una función para extraer y limpiar ubicaciones de los summaries\n",
        "def extract_and_clean_locations(text):\n",
        "    ner_results = ner_pipeline(text)  # Aplicamos el modelo NER al texto\n",
        "    locations = []  # Lista para almacenar las ubicaciones extraídas\n",
        "    for result in ner_results:\n",
        "        if 'LOC' in result['entity']:\n",
        "            word = result['word']\n",
        "            if word.startswith('##'):\n",
        "                # Si el token empieza con ##, se une al token anterior\n",
        "                if locations:\n",
        "                    locations[-1] += word[2:]  # Unimos subpalabra a la palabra anterior\n",
        "                else:\n",
        "                    locations.append(word[2:])  # Si es el primer token, solo eliminamos ##\n",
        "            else:\n",
        "                locations.append(word)\n",
        "\n",
        "    cleaned_locations = ' '.join(locations)  # Unimos las palabras con espacios\n",
        "    return cleaned_locations  # Devolvemos las ubicaciones limpias\n",
        "\n",
        "# Dividimos el dataset en lotes y acumulamos los resultados\n",
        "batch_size = 10  # Tamaño del lote\n",
        "all_extracted_locations = []  # Lista para acumular todas las ubicaciones extraídas y limpiadas\n",
        "\n",
        "# Procesamos el dataset en lotes\n",
        "for start in range(0, len(df), batch_size):\n",
        "    end = min(start + batch_size, len(df))  # Definimos el rango del lote actual\n",
        "    batch_texts = df['Summary'][start:end].tolist()  # Extraemos el texto del lote actual\n",
        "    # Extraemos y limpiamos las ubicaciones del lote actual y acumulamos los resultados\n",
        "    all_extracted_locations.extend([extract_and_clean_locations(text) for text in batch_texts])\n",
        "\n",
        "# Añadimos la columna de ubicaciones extraídas y limpiadas al dataframe\n",
        "df['Extracted_Location'] = all_extracted_locations\n",
        "\n",
        "# Guardamos el dataframe resultante en un archivo CSV\n",
        "output_file_path = 'Extracted_Locations_Clean_2.csv'\n",
        "df.to_csv(output_file_path, index=False)  # Guardamos el dataframe en un archivo CSV sin el índice\n",
        "\n",
        "# Mostramos las primeras filas del dataframe con la nueva columna\n",
        "print(df[['Summary', 'Extracted_Location']].head())\n",
        "\n",
        "# Descargamos el archivo CSV\n",
        "from google.colab import files  # Importamos la biblioteca para descargar archivos en Google Colab\n",
        "files.download('Extracted_Locations_Clean_2.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "i0nhIQZLzkQk",
        "outputId": "36b0f97f-2f64-4ae4-a349-26372a988d5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Summary  \\\n",
            "0  Devin Patrick Kelley, 26, an ex-air force offi...   \n",
            "1  Scott Allen Ostrem, 47, walked into a Walmart ...   \n",
            "2  Radee Labeeb Prince, 37, fatally shot three pe...   \n",
            "3  Stephen Craig Paddock, opened fire from the 32...   \n",
            "4  Jimmy Lam, 38, fatally shot three coworkers an...   \n",
            "\n",
            "              Extracted_Location  \n",
            "0                          Texas  \n",
            "1                  Denver Denver  \n",
            "2  Baltimore Wilmington Delaware  \n",
            "3                  Manadalay Bay  \n",
            "4                  San Francisco  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_09a9a81d-3160-41a8-9f0a-6beffd1d6697\", \"Extracted_Locations_Clean_2.csv\", 145157)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}